#!/usr/bin/env python3
"""
Test script for new standardized metrics calculations.
This script demonstrates the new scoring system without touching the database.
"""

import math
import numpy as np
from datetime import datetime

def calculate_platform_normalized_velocity(videos_found_today: int, platform_baseline: float) -> float:
    """Calculate velocity as percentage of platform's typical daily volume"""
    if platform_baseline <= 0:
        return 0.0
    return round((videos_found_today / platform_baseline) * 100, 1)

def calculate_keyword_relative_acceleration(velocity_history: list, current_velocity: float) -> float:
    """Calculate acceleration as ratio change from keyword's own baseline"""
    if not velocity_history or len(velocity_history) < 2:
        return 1.0  # No acceleration data
    
    baseline_velocity = np.mean(velocity_history)
    
    if baseline_velocity == 0:
        return 2.0 if current_velocity > 0 else 0.5
    
    return round(current_velocity / baseline_velocity, 3)

def calculate_momentum_score(velocity_history: list) -> float:
    """Calculate momentum score (0-100) based on velocity trend"""
    if not velocity_history or len(velocity_history) < 3:
        return 50.0  # Neutral momentum
    
    try:
        # Calculate trend using linear regression
        days = np.arange(len(velocity_history))
        slope, intercept = np.polyfit(days, velocity_history, 1)
        
        # Normalize slope to keyword's average velocity
        avg_velocity = np.mean(velocity_history)
        normalized_slope = slope / avg_velocity if avg_velocity > 0 else 0
        
        # Convert to 0-100 score with sigmoid
        momentum_score = 50 + (50 * math.tanh(normalized_slope * 2))
        
        return round(momentum_score, 1)
    except Exception as e:
        print(f"Error calculating momentum score: {e}")
        return 50.0

def calculate_trend_score(velocity_platform_normalized: float, momentum_score: float) -> float:
    """Calculate unified trend score combining velocity and momentum"""
    # Velocity component (cap at 200 for scoring)
    velocity_capped = min(200, velocity_platform_normalized)
    velocity_score = velocity_capped / 2  # Convert to 0-100 scale
    
    # Weighted combination: 60% velocity, 40% momentum
    trend_score = (0.6 * velocity_score) + (0.4 * momentum_score)
    
    return round(trend_score, 1)

def test_scenarios():
    """Test various scenarios with the new metrics"""
    print("="*80)
    print("YouTube New Standardized Metrics - Test Scenarios")
    print("="*80)
    
    # Platform baseline (example for YouTube)
    platform_baseline = 150.0  # videos/day across all keywords
    
    scenarios = [
        {
            "name": "ChatGPT - Dominant Keyword",
            "videos_found_today": 360,  # 240% of baseline
            "velocity_history": [45, 50, 55, 60, 65, 70, 75]  # Strong upward trend
        },
        {
            "name": "Claude - Growing Keyword", 
            "videos_found_today": 75,   # 50% of baseline
            "velocity_history": [5, 8, 12, 18, 25, 35, 45]   # Explosive growth trend
        },
        {
            "name": "Midjourney - Stable Keyword",
            "videos_found_today": 15,   # 10% of baseline
            "velocity_history": [20, 18, 22, 19, 21, 20, 18]  # Stable, no trend
        },
        {
            "name": "New Tool - Declining",
            "videos_found_today": 3,    # 2% of baseline
            "velocity_history": [25, 20, 15, 12, 8, 5, 3]    # Declining trend
        },
        {
            "name": "Sora - Viral Spike",
            "videos_found_today": 450,  # 300% of baseline
            "velocity_history": [10, 12, 15, 18, 25, 180, 280]  # Sudden viral spike
        }
    ]
    
    for scenario in scenarios:
        print(f"\nüìä {scenario['name']}")
        print("-" * 60)
        
        # Calculate all metrics
        velocity_normalized = calculate_platform_normalized_velocity(
            scenario['videos_found_today'], platform_baseline
        )
        
        acceleration_relative = calculate_keyword_relative_acceleration(
            scenario['velocity_history'], scenario['videos_found_today']
        )
        
        momentum_score = calculate_momentum_score(scenario['velocity_history'])
        
        trend_score = calculate_trend_score(velocity_normalized, momentum_score)
        
        # Raw calculations for context
        raw_velocity = scenario['videos_found_today']
        keyword_baseline = np.mean(scenario['velocity_history'])
        
        # Results
        print(f"Videos found today:           {raw_velocity}")
        print(f"Platform baseline:            {platform_baseline} videos/day")
        print(f"Keyword 7-day average:        {keyword_baseline:.1f} videos/day")
        print()
        print("NEW STANDARDIZED METRICS:")
        print(f"  Velocity (normalized):      {velocity_normalized}% of platform baseline")
        print(f"  Acceleration (relative):    {acceleration_relative:.2f}x keyword baseline")
        print(f"  Momentum score:             {momentum_score:.1f}/100")
        print(f"  Trend score v2:             {trend_score:.1f}/100")
        print()
        
        # Interpretation
        if velocity_normalized > 200:
            vel_interp = "üî• DOMINANT on platform"
        elif velocity_normalized > 100:
            vel_interp = "üìà Above average activity"
        elif velocity_normalized > 50:
            vel_interp = "üìä Below average activity"
        else:
            vel_interp = "üìâ Low activity"
        
        if acceleration_relative > 1.5:
            acc_interp = "üöÄ Explosive growth vs own history"
        elif acceleration_relative > 1.2:
            acc_interp = "‚¨ÜÔ∏è Strong growth vs own history"
        elif acceleration_relative > 0.9:
            acc_interp = "‚û°Ô∏è Stable vs own history"
        else:
            acc_interp = "‚¨áÔ∏è Declining vs own history"
        
        if trend_score > 80:
            trend_interp = "üåü HOT TREND - High priority"
        elif trend_score > 60:
            trend_interp = "üéØ Rising trend - Monitor closely"
        elif trend_score > 40:
            trend_interp = "üìã Normal trend - Regular monitoring"
        else:
            trend_interp = "‚ùÑÔ∏è Cooling trend - Low priority"
        
        print("INTERPRETATION:")
        print(f"  Velocity:     {vel_interp}")
        print(f"  Acceleration: {acc_interp}")
        print(f"  Overall:      {trend_interp}")

def test_cross_platform_comparison():
    """Demonstrate cross-platform comparison capability"""
    print("\n" + "="*80)
    print("Cross-Platform Comparison Example")
    print("="*80)
    
    platforms = {
        "Reddit": {"baseline": 500, "chatgpt_activity": 1200},
        "YouTube": {"baseline": 150, "chatgpt_activity": 300},
        "LinkedIn": {"baseline": 100, "chatgpt_activity": 180}
    }
    
    print("ChatGPT activity across platforms (normalized scoring):")
    print()
    
    for platform, data in platforms.items():
        velocity_score = calculate_platform_normalized_velocity(
            data["chatgpt_activity"], data["baseline"]
        )
        print(f"{platform:10} | {data['chatgpt_activity']:4d} posts | {data['baseline']:3.0f} baseline | {velocity_score:5.1f}% score")
    
    print()
    print("Result: ChatGPT trending strongest on YouTube (200%) despite fewer absolute posts")
    print("This demonstrates the power of platform-normalized scoring!")

if __name__ == "__main__":
    test_scenarios()
    test_cross_platform_comparison()
    
    print("\n" + "="*80)
    print("‚úÖ Test completed! The new standardized metrics are working correctly.")
    print("Key benefits:")
    print("  ‚Ä¢ Platform-normalized velocity enables fair comparison")
    print("  ‚Ä¢ Keyword-relative acceleration shows momentum vs own history")
    print("  ‚Ä¢ Unified trend score combines both for easy ranking")
    print("  ‚Ä¢ Cross-platform comparisons now possible")
    print("="*80)