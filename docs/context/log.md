# YouTube App - Production System Log

## Core Instructions for Coding Agents

You are a coding agent working with a production-ready YouTube scraper system.
You have 2 rules:
**1. No tech debt** - If a problem arises, we provide the long-term solution right away.
**2. No data fabrication** - Errors always preferred over mock data or fallback placeholders.

## Project Overview

- **Project Name**: youtube_app (Alpine-based wget collection)
- **Database**: Firebase Firestore (credentials via file path in .env)
- **Caching**: Upstash Redis with REST API client
- **Runtime**: Python 3.10+ on Alpine Linux VM
- **Deployment**: GitHub Actions auto-deployment on push to main
- **VM**: SSH to VM using `/workspace/droplet1` (private key) - IP: 134.199.201.56
- **Location**: `/opt/youtube_app` on VM
- **Repository**: https://github.com/canaanhowell/youtube-scraper-production

## Current Status (2025-08-06)

### 🚀 **Multi-Instance Collection System Active**

The YouTube scraper is running with a new multi-instance architecture to handle scaling:

✅ **System Status**:
- **VM**: Running at 134.199.201.56 - 4 vCPU, 8GB RAM
- **Project Path**: `/opt/youtube_app/`
- **VPN System**: 3 VPN containers (youtube-vpn-1, youtube-vpn-2, youtube-vpn-3) with staggered access
- **Collection**: 3 instances processing keywords in parallel
- **Firebase**: Connected with proper logging format
- **Redis**: Upstash Redis REST API configured
- **Deployment**: GitHub Actions auto-deployment ACTIVE
- **Analytics Pipeline**: Fully operational with interval and daily metrics
- **Collection Method**: wget-based (20 videos per keyword)
- **Collection Schedule**: Every 10 minutes via cron (staggered multi-instance)

### 🔧 **Latest Updates (2025-08-06)**:

**🎯 Exact Phrase Matching Implementation - DEPLOYED** (Evening):
- ✅ **Enhanced Keyword Filtering**: Implemented exact phrase matching for multi-word keywords
- ✅ **Issue Addressed**: Previous fuzzy matching allowed words in wrong order (e.g., "AI Character" matching "character ai")
- ✅ **Solution**: Updated `_title_contains_keyword()` to require exact phrase matches with proper spacing
- ✅ **Examples**: "character ai" must appear as "Character AI" or "character-ai" (hyphenated variant supported)
- ✅ **Impact**: Eliminates false matches where keyword words appear separately or in wrong order
- ✅ **Configuration**: YOUTUBE_STRICT_TITLE_FILTER=true enabled for exact phrase filtering
- ✅ **Code Updated**: Both wget and Playwright filtering methods use consistent exact matching logic
- ✅ **Deployed**: Active in production, will improve data quality for multi-word keywords

**🎯 Daily Metrics Performance Optimization - DEPLOYED** (Earlier):
- ✅ **8x Performance Improvement**: Optimized `youtube_daily_metrics_unified_vm.py` with range queries
- ✅ **Issue Fixed**: Category aggregations only showing single keywords (e.g., only `qwen3` in ai_coding_agents)
- ✅ **Root Cause**: Individual Firebase document queries (90+ queries per category) causing slowness and incomplete data
- ✅ **Solution**: Replaced with efficient range queries using Firebase `where` clauses for date ranges
- ✅ **Before**: ~5+ minutes runtime with 1,270+ individual queries
- ✅ **After**: ~37 seconds runtime with efficient batch queries per category
- ✅ **Impact**: All keywords now properly appear in category time window aggregations (30_days, 90_days)
- ✅ **Deployed**: Optimized script deployed to VM at `/opt/youtube_app/src/analytics/metrics/`
- ✅ **Verified**: Script tested and working on production VM, ready for next 2:00 AM run
- ✅ **Keywords Fixed**: ai_coding_agents now shows all 7 keywords (claude code, codex, cursor, github copilot, qwen3, testsprite 20, windsurf)
- ✅ **All Categories**: Fix applies to all categories system-wide, not just coding agents

**🎯 Staggered Cron Schedule Implemented**:
- ✅ Replaced simultaneous instance starts with staggered cron entries
- ✅ Instance 1: Runs at :00, :10, :20, :30, :40, :50
- ✅ Instance 2: Runs at :03, :13, :23, :33, :43, :53 (3-minute offset)
- ✅ Instance 3: Runs at :06, :16, :26, :36, :46, :56 (6-minute offset)
- ✅ Interval metrics: Runs at :09, :19, :29, :39, :49, :59 (after all complete)
- ✅ Reduces server load by spreading instances across 6 minutes
- ✅ Each instance gets dedicated time without resource competition

**🎯 Log Cleanup System Added**:
- ✅ Created automated cleanup for collection logs older than 5 days
- ✅ Interactive script: `cleanup_old_collection_logs.py` for manual cleanup
- ✅ Automated script: `cleanup_old_logs_auto.py` for cron jobs
- ✅ Weekly cleanup cron: Sundays at 3 AM UTC
- ✅ Maintains database performance by removing old logs
- ✅ Logs cleanup statistics to `youtube_maintenance_logs` collection

**🎯 Interval Metrics Logging Fix**:
- ✅ Fixed interval metrics creating hash IDs in collection logs
- ✅ Updated to use Firebase client's `log_collection_run` method
- ✅ Ensures consistent timestamp-based document IDs
- ✅ Cleaned up 24 hash documents from interval metrics

### 🔧 **Previous Updates (2025-08-05 Evening)**:

**🎯 Multi-Instance Collection System - WORKING**:
- ✅ Implemented 3-instance parallel collection to handle keyword scaling
- ✅ Created docker-compose-multi.yml with 3 VPN containers (ports 8000, 8003, 8004)
- ✅ Dynamic keyword distribution across instances (currently 5-5-6 split for 16 keywords)
- ✅ Created youtube_collection_manager_simple.py for simpler VPN handling
- ✅ Process locking prevents overlapping runs of same instance
- ✅ Fixed Firebase logging format with proper keywords_processed array and videos_per_keyword map
- ✅ Added update_keyword_timestamp method to FirebaseClient
- ✅ **CRITICAL FIX**: Added instance-specific Redis key namespacing to prevent duplicate detection between parallel instances
- ✅ System now collecting proper video counts (ChatGPT: 20 videos vs previous 5)
- ✅ Ready to scale to 40+ keywords without collection overlaps

**🎯 Collection Issue Resolution**:
- **Initial Problem**: Keywords increased causing collections to take >10 minutes
- **Root Cause 1**: Multiple instances overlapping and fighting over single VPN container
- **Root Cause 2**: Shared Redis cache causing false duplicates between parallel instances
- **Solution**: 3 parallel instances with dedicated VPN containers + instance-specific Redis namespacing
- **Result**: Each instance collects proper video counts, system ready for large-scale keyword growth

### 🔧 **Earlier Updates (2025-08-05)**:

**🎯 Collection Logs Hash ID Fix**:
- ✅ Identified root cause: collection_logger.py was using session_id as document ID
- ✅ Fixed all Firebase client implementations to validate timestamp-based IDs
- ✅ Updated collection_logger.py to generate proper timestamp IDs (collection_YYYY-MM-DD_HH-MM-SS_UTC)
- ✅ Added ID validation to prevent future hash ID creation
- ✅ Created monitoring tools to detect and clean up hash IDs
- ✅ All collection logs now use consistent readable timestamp format

**🎯 Critical Fixes - Video Storage & Keywords**:
- ✅ Fixed video storage issue - Firestore requires parent documents for subcollections
- ✅ Created missing parent documents for all 16 keywords
- ✅ Updated scraper to auto-create parent documents before saving videos
- ✅ Synchronized all keywords across collections using reddit_keywords as baseline
- ✅ Merged duplicate video collections (stable_diffusion, leonardo_ai, runway)
- ✅ Fixed collection schedule to run every 10 minutes with interval metrics
- ✅ Cleaned up 95+ hash document IDs in youtube_collection_logs

**🎯 YouTube Filter Fix**:
- ✅ Fixed YouTube filter from `sp=EgQIARAB` to `sp=CAISBAgBEAE%253D`
- ✅ Now properly sorts by upload date within last hour
- ✅ Dramatically improves relevance of collected videos

### 🔧 **Previous Updates (2025-08-05)**:

**🎯 Project Renaming** (Latest - 2025-08-05):
- ✅ Renamed from `wget_youtube_scraper` back to `youtube_app`
- ✅ Confirmed wget method captures 20 videos per keyword
- ✅ Updated all references and paths throughout the codebase
- ✅ Moved all Python scripts from root to src/ directories
- ✅ Updated all deployment scripts for new paths
- ✅ Cleaned up root directory - only config files remain
- ✅ Organized Python scripts in src/ directories

### 🔧 **Previous Updates (2025-08-04)**:

**🎯 Platform Baseline System Simplified** (Latest):
- ✅ Removed complex platform baseline calculation script
- ✅ Implemented hardcoded platform baseline approach for simplicity
- ✅ Created set_platform_baseline.py for manual baseline management
- ✅ Set YouTube platform baseline to 150.0 videos/day (hardcoded)
- ✅ Updated all documentation to reflect hardcoded approach
- ✅ Verified velocity calculations working correctly with hardcoded baseline
- ✅ System deployed and operational on production VM

**🎯 Firebase Schema v2.0 Migration**:
- ✅ Successfully migrated Firebase schema to v2.0 standardized metrics
- ✅ Converted daily_metrics from subcollection to map field for all 15 keywords
- ✅ Updated 566 category snapshot documents with new field names
- ✅ Transformed field names: videos_found_in_day → new_videos_in_day, views_count → total_views
- ✅ Added standardized v2.0 fields: velocity (platform-normalized %), acceleration (keyword-relative ratio)
- ✅ Removed legacy metrics and cleaned up keyword document structure
- ✅ Updated youtube_daily_metrics_unified_vm.py to write new schema format
- ✅ Added current_velocity field updates to keywords for real-time tracking
- ✅ All production systems now using v2.0 schema with backward compatibility removed

**🎯 Standardized Metrics v2.0 Implementation**:
- ✅ Implemented platform-normalized velocity scoring system
- ✅ Added keyword-relative acceleration calculations
- ✅ Created momentum score (0-100) based on trend analysis
- ✅ Built unified trend score v2 combining velocity + momentum
- ✅ Enhanced youtube_daily_metrics_unified_vm.py with new scoring
- ✅ Updated category snapshots with standardized metrics
- ✅ Created platform baseline calculator for YouTube
- ✅ Added platform_metrics collection for baseline storage
- ✅ Updated firestore_mapping.md with v2.0 schema
- ✅ Comprehensive testing validated all calculations
- ✅ Cross-platform comparison now possible with normalized scores

**Key Benefits of New Metrics System**:
- 🔥 **Platform-Normalized Velocity**: 150% = 150% of YouTube platform average
- 🚀 **Keyword-Relative Acceleration**: 1.5x = 150% vs keyword's own baseline
- 📈 **Momentum Score**: 0-100 trend momentum using linear regression
- 🎯 **Unified Trend Score**: Combined ranking score (60% velocity + 40% momentum)
- 🌐 **Cross-Platform**: Standardized scoring enables comparison across platforms

**Scheduled Function Paths Fixed**:
- ✅ Fixed cron_scraper_with_metrics.sh to use correct script paths
- ✅ Updated from module import to direct script execution
- ✅ All scheduled functions now pointing to reorganized project structure
- ✅ Verified scripts are executable and working

**Interval Metrics Timing Fixed**:
- ✅ Fixed interval metrics running every 5 minutes instead of hourly
- ✅ Disabled systemd analytics timer that was causing excessive runs
- ✅ Integrated interval metrics into hourly scraper cron job
- ✅ Now runs correctly: Scraper at :15, then interval metrics immediately after
- ✅ Proper data flow: Videos collected → Interval metrics calculated → Daily metrics aggregated

**Analytics Pipeline Fixed**:
- ✅ Fixed systemd service configuration for analytics
- ✅ Daily metrics cron job verified (runs at 2:00 AM daily)
- ✅ Interval metrics now calculating properly after each scraper run
- ✅ Created fix scripts for future troubleshooting
- ✅ All metrics services operational

**Video Collection Confirmed Working**:
- ✅ Videos ARE being collected successfully (56 videos in last run)
- ✅ Strict title filter disabled to improve collection rates
- ✅ Data properly stored in `youtube_videos/{keyword}/videos/`
- ✅ Interval metrics stored in `youtube_keywords/{keyword}/interval_metrics/`

### 🔧 **Previous Updates (2025-01-03)**:

**Title Filtering Enhancement** (Latest):
- ✅ Added YOUTUBE_STRICT_TITLE_FILTER feature
- ✅ Only collects videos containing the search keyword in their title
- ✅ Defaults to true for improved data quality
- ✅ Environment variable: `YOUTUBE_STRICT_TITLE_FILTER=true`
- ✅ Reduces irrelevant data collection significantly

**Simplified Deployment Process**:
- ✅ 3-phase deployment process implemented
- ✅ No Git operations on production VM
- ✅ Artifact-based deployment for cleaner production
- ✅ Automated health checks and verification
- ✅ Zero-downtime deployments

**Deployment Complete** (13:00 UTC):
- Successfully deployed to VM via GitHub push
- Fixed all hardcoded paths from `/opt/youtube_scraper` to `/opt/youtube_app`
- Python virtual environment configured
- All dependencies installed
- Credentials properly configured

**Hourly Automation** (13:27 UTC):
- Cron job configured to run hourly at :15 past the hour
- Logs available at `/opt/youtube_app/logs/cron.log`
- Script: `/opt/youtube_app/cron_scraper.sh`

**Auto-Deployment Working**:
- GitHub Actions workflow active
- Push to main branch = automatic deployment
- Smart deployment detects changed files
- Automatic backup before updates
- Path fixes deployed and verified

**Current Issues Resolved**:
- ✅ Path migration completed
- ✅ Environment variables corrected (SURFSHARK_PRIVATE_KEY, SURFSHARK_ADDRESS)
- ✅ Firebase credentials deployed
- ✅ Logs directory created
- ✅ Hourly automation via cron job
- ✅ Title filtering implemented for better data quality

## Key Features

### 🎯 **Deployment Process**
1. **Push to GitHub** → Triggers auto-deployment
2. **Smart Detection** → Only updates changed components
3. **Auto-Configure** → New services detected and configured
4. **Backup First** → Automatic backup before changes
5. **Auto-Rollback** → If deployment fails

### 📁 **Project Structure**
```
youtube_app/
├── src/
│   ├── scripts/
│   │   ├── youtube_collection_manager.py  # Main orchestrator
│   │   ├── youtube_scraper_production.py  # Core scraping logic
│   │   └── collectors/
│   │       └── run_analytics.py           # Analytics runner
│   ├── utils/                             # Utilities
│   │   ├── env_loader.py                  # Fixed for youtube_app paths
│   │   ├── logging_config_enhanced.py     # Dynamic log paths
│   │   └── firebase_client_enhanced.py
│   └── analytics/                         # Analytics pipeline
│       └── metrics/
│           ├── youtube_keywords_interval_metrics.py
│           └── youtube_daily_metrics_unified_vm.py
├── deployment/
│   ├── scripts/
│   │   ├── smart_deploy.sh          # Smart deployment script
│   │   ├── service_detector.py      # Auto-detect services
│   │   └── backup_manager.py        # Backup/rollback
│   └── test_deployment.py           # Deployment verification
├── .github/
│   └── workflows/
│       └── auto-deploy.yml          # GitHub Actions
└── .env                             # Credentials (gitignored)
```

### 🔒 **Security**
- `.env` file gitignored
- Firebase credentials file gitignored  
- Credentials added manually on VM after deployment
- VPN for anonymity (VM-only)

## Usage Instructions

### 🚀 **Deploy to Production**
```bash
# 1. Push to GitHub (triggers deployment)
git push origin main

# 2. Monitor deployment
# Check GitHub Actions: https://github.com/canaanhowell/youtube-scraper-production/actions

# 3. SSH to VM and add credentials (first time only)
ssh -i /workspace/droplet1 root@134.199.201.56
cd /opt/youtube_app
vim .env  # Add production credentials
```

### ⚙️ **Configure Title Filtering**
```bash
# In .env file, set title filtering (defaults to true)
YOUTUBE_STRICT_TITLE_FILTER=true  # Only collect videos with keyword in title
YOUTUBE_STRICT_TITLE_FILTER=false # Collect all videos from search results
```

### 📊 **Monitor System**
```bash
# Check cron job status
crontab -l

# View collection logs for all instances
tail -f logs/collector_1.log logs/collector_2.log logs/collector_3.log

# View main scraper log
tail -f logs/scraper.log

# View multi-instance cron log
tail -f logs/cron_multi.log

# View analytics logs
tail -f logs/analytics.log

# View daily metrics logs
tail -f logs/daily_metrics.log

# Check VPN containers status
docker ps | grep youtube-vpn

# Check systemd timers
systemctl list-timers --all | grep youtube

# Check deployment log
tail -f /var/log/youtube_deploy.log

# Manual backup/rollback if needed
python3 deployment/scripts/backup_manager.py backup
python3 deployment/scripts/backup_manager.py rollback

# Run daily metrics manually if needed
bash deployment/scripts/run_daily_metrics_now.sh

# Test individual instance
cd /opt/youtube_app && source venv/bin/activate
python src/scripts/youtube_collection_manager_simple.py --instance 1
```

## Important Notes

### ⚠️ **Cannot Test Locally**
- VPN/Docker required (only on VM)
- Local testing limited to Firebase connection
- Full testing requires VM environment

### ✅ **Verification Checklist - MUST CHECK BEFORE DECLARING SUCCESS**
When implementing any collection system changes, verify ALL of the following:

1. **Process Running**: Check that collection processes start without errors
   ```bash
   tail -f logs/collector_*.log
   # ✓ Should see "Starting collection" messages
   ```

2. **VPN Connected**: Verify VPN containers are healthy
   ```bash
   docker ps | grep youtube-vpn
   # ✓ Should show "healthy" status for all 3 containers
   ```

3. **Videos Actually Collected**: Check Firebase for actual video data
   ```bash
   # Check collection logs in Firebase
   # ✓ total_videos_collected should be > 0 (target: 40+ videos per run)
   # ✓ videos_per_keyword should show 15-20 for active keywords like ChatGPT, Claude
   ```

4. **No wget Errors**: Verify wget is successfully fetching pages
   ```bash
   grep "Failed to fetch" logs/collector_*.log
   # ✓ Should return no results or very few
   ```

5. **Firebase Verification**: Check actual video documents exist
   ```bash
   # In Firebase console: youtube_videos/{keyword}/videos/
   # ✓ Should see new video documents with recent timestamps
   ```

6. **Redis Deduplication Working**: Verify instances aren't interfering
   ```bash
   # Each instance should show proper video counts, not excessive duplicates
   # ✓ Instance logs should show reasonable duplicate ratios (not 90%+ duplicates)
   ```

**⚠️ NEVER declare success without verifying actual data collection AND proper video counts!**

### 📝 **Required Environment Variables**
The `.env` file on VM must include:
```env
# Firebase
GOOGLE_SERVICE_KEY_PATH=/opt/youtube_app/ai-tracker-466821-892ecf5150a3.json
FIRESTORE_PROJECT_ID=ai-tracker-466821

# Redis
UPSTASH_REDIS_REST_URL=https://your-redis.upstash.io
UPSTASH_REDIS_REST_TOKEN=your-token

# VPN WireGuard Configuration
SURFSHARK_PRIVATE_KEY=your-wireguard-private-key
SURFSHARK_ADDRESS=10.14.0.2/16

# Environment
ENVIRONMENT=production
LOG_LEVEL=INFO

# YouTube Settings
YOUTUBE_STRICT_TITLE_FILTER=true  # Only collect videos with keyword in title (default: true)
```

## System Architecture

### **Production Components**:
- **YouTube Scraper**: wget-based scraper with VPN rotation
- **VPN Manager**: WireGuard integration with 24 US Surfshark servers
- **Data Storage**: Firebase Firestore for keywords and results
- **Cache Layer**: Upstash Redis REST API
- **Analytics Pipeline**: Daily metrics and trend analysis
- **Deployment**: GitHub Actions with smart detection

### **Key Commands**:
```bash
# Run collection manually
cd /opt/youtube_app && source venv/bin/activate
python src/scripts/youtube_collection_manager.py

# Check logs
tail -f /opt/youtube_app/logs/scraper.log

# Check cron logs
tail -f /opt/youtube_app/logs/cron.log

# Test with limited keywords
python src/scripts/youtube_collection_manager.py --test

# View next scheduled run
systemctl list-timers --all | grep youtube

# Set platform baseline (manual management)
python src/analytics/metrics/set_platform_baseline.py --baseline 150.0

# View current platform baseline
python -c "from src.utils.firebase_client import FirebaseClient; fb = FirebaseClient(); doc = fb.db.collection('platform_metrics').document('youtube').get(); print(f'Current baseline: {doc.to_dict().get(\"daily_baseline\", \"NOT FOUND\")}' if doc.exists else 'No baseline found')"

# Test new standardized metrics
python test_new_metrics.py

# Monitor for hash document IDs in collection logs
python src/scripts/utilities/monitor_collection_logs.py

# Check and clean up hash IDs if found
python check_and_fix_collection_logs.py

# Clean up any remaining hash document IDs
python cleanup_hash_logs.py
```

## Summary

The wget YouTube scraper is now:
- ✅ Fully deployed to production VM
- ✅ Auto-deployment enabled and tested
- ✅ All paths updated to `/opt/youtube_app`
- ✅ Environment variables properly configured
- ✅ Ready for production data collection
- ✅ Running hourly via cron job at :15 past each hour
- ✅ Analytics pipeline operational (interval metrics run immediately after scraper)
- ✅ Daily metrics calculating at 2:00 AM daily with **standardized v2.0 metrics** and **8x performance optimization**
- ✅ Platform baseline calculator for velocity normalization
- ✅ Cross-platform comparable metrics system
- ✅ All systemd services configured and active

### Active Services:
- **Multi-Instance Collection**: Staggered schedule every 10 minutes (individual cron entries)
  - Instance 1: Runs at :00 (youtube-vpn-1 container)
  - Instance 2: Runs at :03 (youtube-vpn-2 container)  
  - Instance 3: Runs at :06 (youtube-vpn-3 container)
  - Dynamic keyword distribution across instances
- **Interval Metrics**: Runs at :09 (after all instances complete)
- **Daily Metrics v2.0**: 2:00 AM daily (cron) - `/opt/youtube_app/deployment/cron_daily_metrics.sh` - **OPTIMIZED: 8x faster with range queries**
- **Weekly Log Cleanup**: Sundays at 3 AM UTC - removes logs >5 days old
- **Platform Baseline**: Hardcoded at 150.0 videos/day (managed via `src/analytics/metrics/set_platform_baseline.py`)
- **Analytics Timer**: DISABLED (was causing metrics to run every 5 minutes)

Any push to GitHub main branch automatically deploys to production!

## Critical Fixes Applied (August 5, 2025)

### Video Storage Fix
- **Issue**: Videos were being counted but not stored in Firestore
- **Root Cause**: Firestore requires parent documents to exist before adding to subcollections
- **Solution**: Created all 16 missing parent documents and updated scraper to auto-create them
- **Impact**: All videos now properly save to database

### Keyword Synchronization
- **Issue**: Mismatched keywords across collections (quotes, different IDs, duplicates)
- **Solution**: Synchronized all collections using reddit_keywords as baseline
- **Merges Completed**:
  - `"chatgpt"` → `chatgpt` (removed duplicate with quotes)
  - `leonardo ai` → `leonardo_ai` (696 videos)
  - `stable diffusion` → `stable_diffusion` (620 videos)
  - `Runway` → `runway` (686 videos)
- **Result**: All 16 keywords now match exactly across reddit_keywords, youtube_keywords, and youtube_videos

### Collection Schedule Update
- **Changed**: From hourly to every 10 minutes
- **Fixed**: Interval metrics now properly runs after each scraper execution
- **Filter**: Updated to `sp=CAISBAgBEAE%253D` for proper "last hour + sort by upload date"

### Data Cleanup
- **Removed**: 95+ hash document IDs from youtube_collection_logs
- **Fixed**: All collection logs now use readable timestamp IDs
- **Updated**: Cron script to ensure both scraper and interval metrics run together

### All YouTube Aggregation Fix
- **Issue**: youtube_categories/all_youtube document wasn't being updated by daily metrics
- **Root Cause**: Daily metrics script only processed categories with keywords, not aggregate views
- **Solution**: Added _update_all_youtube_snapshot method to create platform-wide aggregation
- **Impact**: Now creates all_youtube snapshots combining all keywords across all categories

### Collection Logs Hash ID Fix
- **Issue**: Documents in youtube_collection_logs were being created with auto-generated hash IDs
- **Root Cause**: collection_logger.py was using session_id as document ID instead of timestamp format
- **Solution**: 
  - Updated collection_logger.py to generate timestamp-based IDs (collection_YYYY-MM-DD_HH-MM-SS_UTC)
  - Added validation to all Firebase client methods to ensure proper ID format
  - Created monitoring and cleanup tools to detect and remove hash IDs
- **Impact**: All collection logs now use consistent, readable timestamp-based document IDs