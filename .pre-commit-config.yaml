# Pre-commit hooks configuration for YouTube Scraper
# Install: pip install pre-commit && pre-commit install

repos:
  # Code formatting
  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3.11
        args: [--line-length=88]
        
  # Import sorting
  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        args: [--profile=black, --line-length=88]
        
  # Linting
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
        args: [--max-line-length=88, --extend-ignore=E203,W503]
        additional_dependencies:
          - flake8-docstrings
          - flake8-import-order
          - flake8-bugbear
          
  # Security scanning
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.5
    hooks:
      - id: bandit
        args: [-r, src/]
        exclude: tests/
        
  # Type checking
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        args: [--ignore-missing-imports, --no-strict-optional]
        additional_dependencies: [types-requests, types-redis]
        exclude: tests/
        
  # General code quality
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      # Basic file checks
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-toml
      - id: check-xml
      
      # Python-specific checks
      - id: check-ast
      - id: check-builtin-literals
      - id: check-docstring-first
      - id: debug-statements
      - id: name-tests-test
        args: [--pytest-test-first]
        
      # Security checks
      - id: detect-private-key
      - id: check-added-large-files
        args: [--maxkb=1000]
        
      # File format checks
      - id: check-case-conflict
      - id: check-executables-have-shebangs
      - id: check-shebang-scripts-are-executable
      
  # Dockerfile linting
  - repo: https://github.com/hadolint/hadolint
    rev: v2.12.0
    hooks:
      - id: hadolint-docker
        args: [--ignore, DL3008, --ignore, DL3009]
        
  # Shell script linting
  - repo: https://github.com/shellcheck-py/shellcheck-py
    rev: v0.9.0.6
    hooks:
      - id: shellcheck
        args: [-e, SC2086, -e, SC2181]
        
  # Commitizen for conventional commits
  - repo: https://github.com/commitizen-tools/commitizen
    rev: v3.13.0
    hooks:
      - id: commitizen
        stages: [commit-msg]
        
  # Local hooks for project-specific checks
  - repo: local
    hooks:
      # Test imports
      - id: test-imports
        name: Test Python imports
        entry: python
        language: system
        args: [-c, "import sys; sys.path.insert(0, '.'); from src.analytics.metrics.keywords_interval_metrics import *; from src.utils.firebase_client_enhanced import *; print('✓ All imports successful')"]
        pass_filenames: false
        
      # Environment file check
      - id: check-env-example
        name: Check .env.example completeness
        entry: python
        language: system
        args: [-c, "
import os;
required_vars = ['ENVIRONMENT', 'GOOGLE_SERVICE_KEY_PATH', 'UPSTASH_REDIS_REST_URL'];
env_example = open('environments/.env.example').read();
missing = [v for v in required_vars if v not in env_example];
assert not missing, f'Missing variables in .env.example: {missing}'
"]
        files: environments/\.env\.example$
        pass_filenames: false
        
      # Configuration validation
      - id: validate-category-mapping
        name: Validate category mapping JSON
        entry: python
        language: system
        args: [-c, "
import json;
with open('src/config/category_mapping.json') as f:
    data = json.load(f);
    assert 'ph_to_youtube_reddit_mapping' in data, 'Missing ph_to_youtube_reddit_mapping key';
    assert len(data['ph_to_youtube_reddit_mapping']) > 0, 'Empty category mapping';
print('✓ Category mapping is valid')
"]
        files: src/config/category_mapping\.json$
        pass_filenames: false
        
      # Analytics entry point test
      - id: test-analytics-entry-point
        name: Test analytics entry point
        entry: python
        language: system
        args: [src/scripts/collectors/run_analytics.py, --help]
        pass_filenames: false
        verbose: true
        
      # Check for sensitive data
      - id: check-sensitive-data
        name: Check for sensitive data in code
        entry: python
        language: system
        args: [-c, "
import re, sys;
patterns = [
    r'(api[_-]?key|password|secret|token)\s*[:=]\s*['\"]?[a-zA-Z0-9]{10,}',
    r'(firebase|redis|surfshark).*['\"][a-zA-Z0-9]{20,}['\"]',
    r'AKIA[0-9A-Z]{16}',  # AWS access key
    r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}'  # UUID patterns
];
content = sys.stdin.read();
for pattern in patterns:
    if re.search(pattern, content, re.IGNORECASE):
        print(f'WARNING: Potential sensitive data found matching pattern: {pattern}');
        sys.exit(1)
"]
        types: [python]
        
      # Check deployment scripts are executable
      - id: check-deployment-scripts
        name: Check deployment scripts are executable
        entry: bash
        language: system
        args: [-c, "find deployment/scripts -name '*.sh' -not -executable -print | wc -l | grep -q '^0$'"]
        pass_filenames: false

# Configuration
default_stages: [commit, push]
fail_fast: false
minimum_pre_commit_version: 3.0.0

# Exclude patterns
exclude: |
  (?x)^(
      \.git/.*|
      \.venv/.*|
      venv/.*|
      __pycache__/.*|
      \.pytest_cache/.*|
      logs/.*|
      backups/.*|
      \.env.*|
      node_modules/.*
  )$